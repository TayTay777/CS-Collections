Why study programming languages (PLs)?  Knowing multiple PLs is good. { Every PL does not have every feature. { There are dierent paradigms. { Dierent PLs are good for dierent tasks.  Knowing the theory behind their design is good. 1.2 There are dierent \levels" of PL  Microcode languages  Machine languages pub/sum/x86_64/sum.s  Assembly languages  Intermediate/Internal PLs (e.g., Java byte code)  High-level PLs (e.g., Java) 1.3 Why are there so many PLs?  Evolution: we've learned better ways of doing things over time.  Socio-economic factors: proprietary interests and commercial advantage.  Some PLs are oriented toward special purposes.  Some PLs are oriented toward special hardware.  We have diverse ideas about what is pleasant to use.  Some PLs are easier to learn.  Some PLs are easier to implement a translator for.  Newer PLs enforce programmer behaviors that reduce program-maintenance costs.  Translation technology has improved.  Some PLs match problem domains.  Expressive power is important, even if computability power is equal.  Some PLs have a huge base of previously written programs and libraries. 1.4 What makes a PL successful?  Some are easy to learn (BASIC, Pascal, LOGO, Scheme).  Some are more \expressive," easier to use once uent, and \powerful" (C, Common Lisp, APL, Algol-68, Perl).  Some are easy to implement (Pascal, BASIC and Forth).  Some can compile to very good (fast/small) code (Fortran).  Some have the backing of a powerful sponsor (COBOL, PL/I, Ada, and Visual Basic)  Some enjoyed wide dissemination at minimal cost (Pascal, Turing, and Java). 1.5 Why do we have PLs? What is a PL for?  A PL (or any language) helps determine the way you think, express algorithms, and solve problems.  Some PLs try to match the user's point of view.  Some PLs try to match the developer's point of view.  They are an abstraction of a machine or virtual machine.  They help you specify what you want the hardware to do without getting down into the bits. 1.6 Why study PLs? (1 of 6)  Knowing multiple PLs makes it easier to choose an appropriate PL: { C versus C++ versus Modula-3 for systems programming { Fortran versus APL versus Ada for numerical computations { C versus Ada for embedded systems { Common Lisp versus Scheme versus ML for symbolic data manipulation { Java versus C/CORBA versus C/COM for networked programs 1.7 Why study PLs? (2 of 6)  Knowing multiple PLs makes it easier to learn new PLs, because many are similar. This is true for natural languages, too.  PL concepts have even more similarity. For example, most PLs have iteration, recursion, and abstraction. They might only dier in syntax and semantic details. 1.8 Why study PLs? (3 of 6)  It helps you use your PL more eectively, and understand its obscure features. For example: { In C/C++, you can understand unions, arrays, pointers, separate compilation, varargs, and exception handling. { In Lisp/Scheme, you can understand rst-class functions/closures, streams, exception handling, and symbol internals. For example: pub/etc/Y.scm pub/etc/Y.sh 1.9 Why study PLs? (4 of 6)  It helps you understand implementation costs and choose between alternative ways of doing things, based on knowledge of what will be done underneath. For example: { Use x<<1 instead of x*2, or x*x instead of x**2. { Use C pointers or Pascal's with statement, to factor address calculations. { Avoid call by value with large data items in Pascal. { Avoid call by name in Algol 60. { Choose between computation and table lookup. 1.10 Why study PLs? (5 of 6)  It helps you gure out how to do things in PLs that don't support them explicitly. For example: { Old Fortran lacks suitable control structures, but you can use gotos, comments, and programmer discipline. { Some PLs lack recursion (e.g., Fortran and CSP), but you can write a recursive algorithm and use mechanical recursion elimination. 1.11 Why study PLs? (6 of 6)  It helps you gure out how to do things in languages that don't support them explicitly. For example: { Fortran lacks named constants and enumerations, but you can use variables that are initialized once, then never changed. { C and Pascal lack modules, but you can use comments and programmer discipline. { Most PLs lack iterators, but you can fake them with functions or member functions. 1.12 PL paradigms (1 of 5)  imperative { von Neumann (e.g., Fortran, Pascal, Basic, and C) { object-oriented (e.g., Smalltalk, Eiel, C++, C#, and Java) { scripting languages (e.g., Bash, AWK, Perl, Python, JavaScript, and PHP)  declarative { functional (e.g., Scheme, ML, Lisp, and FP) { logic, constraint-based (e.g., Prolog, VisiCalc, and RPG) 1.13 PL paradigms (2 of 5)  You know a few imperative PLs. These PLs use multiple-assignment variables. An assignment statement can cause a side-eect, which in uences future computation.  An imperative program tells the computer how to solve a problem in a particular way (e.g., Newton's, or the Babylonian, method for computing the square root of a number). 1.14 PL paradigms (3 of 5)  You are not expected to know any declarative PLs. A declarative program tells the computer what problem to solve. It describes all acceptable solutions. This is especially true of the logic PLs (e.g., Prolog). For example: sqrt(x2) = x   is a declarative program for computing an approximation to the square root of a number. The PL translator is free to choose any way of nding an acceptable solution.  SQL and Make can be placed in this family. 1.15 PL paradigms (4 of 5)  Functional PLs are based on recursive-function denition and invocation, without side-eects.  You can see that these families are not always clear cut: You can write a C program in a functional style, if you constrain yourself. 1.16 PL paradigms (5 of 5)  Data ow PLs model computation with nodes that transform input data streams into output data streams. Arcs between nodes model the streams.  Scripting PLs were originally used to glue-together other programs (e.g., sh, the Unix shell). They've morphed into von Neumann and OO PLs.  OO PLs model computation as the message-passing simulation of real-world entities. The other imperative PLs are identied with John von Neumann. 1.17 Euclid's greatest common divisor algorithm  C pub/ch1/gcd/gcd.c  Scheme pub/ch1/gcd/gcd.scm  Prolog pub/ch1/gcd/gcd.pl 1.18 Compilation versus interpretation (1 of 5)  These are not opposites, and, sometimes, there is not a clear-cut distinction.  Pure compilation: The compiler translates the high-level source program into an equivalent target program, perhaps in assembly language, and then exits.  Pure interpretation: After the interpreter translates each statement or construct, it immediately executes the translation.  Compiled programs execute with higher performance: time and space.  Interpretation is more exible and provides a better debugging environment. 1.19 Compilation versus interpretation (2 of 5)  Compilation  Interpretation  Hybrid 1.20 Compilation versus interpretation (3 of 5)  C is typically compiled, Lisp is typically interpreted, and Java is typically a hybrid.  With compilation, performance (i.e., speed) may be ten times better.  The Java byte-code interpreter (i.e., the JVM) is a virtual machine.  The intermediate program (e.g., byte code) is also called intermediate code. 1.21 Compilation versus interpretation (4 of 5)  Compilation does not have to produce machine language for some sort of hardware.  Compilation is translation from one PL into another, with full analysis of the meaning of the input.  Compilation entails semantic understanding of what is being processed.  Preprocessing is a textual, rather than grammatical, transformation. 1.22 Compilation versus interpretation (5 of 5)  Many compiled languages have interpreted pieces. For example, Fortran has FORMAT statements, and C has printf/scanf function calls.  Most use \virtual instructions": { set operations in Pascal { string manipulation in Basic  Some compilers produce nothing but virtual instructions (e.g., Pascal P-code, Java byte code, and Microsoft COM+). 1.23 Implementation strategies (1 of 7)  Some translation systems employ a preprocessor: { removes comments and white space { groups characters into tokens (keywords, identiers, numbers, and punctuation) { expands abbreviations in the style of a macro assembler { may identify higher-level syntactic structures 1.24 Implementation strategies (2 of 7)  Many translation systems (e.g., GCC) employ an assembler and linker: 1.25 Implementation strategies (3 of 7)  The C preprocessor has several features: { conditional compilation: #if and friends { le inclusion: #include { macro denition and expansion: #define { pragmas { line control 1.26 Implementation strategies (4 of 7)  Source-to-source translation: 1.27 Implementation strategies (5 of 7)  Porting a PL like Pascal to a new computer: { Participants: A pcode interpreter (assembly) B pcode interpreter (binary) C pascal-to-pcode compiler (pcode) D pascal-to-assembly compiler (assembly) E pascal-to-assembly compiler (binary) F pascal-to-assembly compiler (pascal) G pascal-to-assembly compiler (pcode) { Possible steps: pub/ch1/bootstrap.pdf 1.28 Implementation strategies (6 of 7)  Compiling typically interpreted PLs: { Delay some operations until run time. { Such operations are often in a library. { In a pinch, revert to the interpreter.  A virtual machine can perform a second kind of compilation, called Just-In-Time (JIT or ORC) compilation. The resulting machine code is much faster.  A Lisp or Prolog program can invoke the translator, to process dynamically created source code. 1.29 Implementation strategies (7 of 7)  There are two other (opposing) variations: { A processor's machine language can be an otherwise virtual machine language (e.g., Java byte code). { A processor's machine language might be interpreted internally:  Machine-instructions are implemented, within the CPU, in rmware.  The interpreter is written in lower-level instructions (i.e., microcode), stored in read-only memory and executed by hardware.  This is common, these days, but allowing users to change the microcode is uncommon (e.g., AMD 2900 bit-slice processors). 1.30 Unconventional compilers  structure editors  pretty printers  syntax highlighters  static analyzers  text formatters  documentation generators  database-query interpreters  debuggers  silicon compilers 1.31 Other programming-environment tools (1 of 2)  documentation (e.g., web, javadoc, and doxygen)  design documents (e.g., dia)  testing (e.g., JUnit and dejagnus)  edit/compile/debug cycle (e.g., emacs and eclipse)  pretty printing and syntax highlighting (e.g., emacs, vim, cb, and vgrind)  cross referencing (e.g., cxref, web, javadoc, and doxygen)  static analysis (e.g., lint and FindBugs)  version control (e.g., rcs, cvs, subversion, and git)  building (e.g., make, ant, and maven) 1.32 Other programming-environment tools (2 of 2)  report generators (e.g., grg: GNU Report Generator)  screen, GUI, and application generators (e.g., screengen: Java Screen Generator)  frameworks and libraries (e.g., X11 and Swing)  debugging (e.g., gdb and ddd, objdump, and readelf)  dynamic analysis (e.g., valgrind and strace)  bug tracking (e.g., bugzilla and GNATS) 1.33 Phases of compilation  Lexical analysis (scanning) inputs a sequence of characters and outputs a sequence of tokens.  Syntax analysis (parsing) outputs a syntax or parse tree.  This is the front-end/back-end border.  Semantic analysis (type checking) outputs a modied tree.  Intermediate code generation outputs an intermediate representation of the target program.  Machine-independent code optimization outputs modied intermediate code.  Code generation outputs a target program.  Machine-dependent code optimization outputs a modied target program. 1.34 Scanning (1 of 2)  Adjacent characters are grouped into tokens: the smallest meaningful units in a program.  The next phase, parsing, only knows about tokens.  Example tokens are: { numeric literals { character literals { string literals { keywords { identiers { operators (e.g., + and ++)  Comments and whitespace are typically not tokens! 1.35 Scanning (2 of 2)  Scanning is recognition of a regular language, the strings of which match regular expressions.  Theoretically, a scanner is a deterministic nite automaton (DFA): a machine that changes from state to state as it consumes input symbols (e.g., characters). 1.36 Parsing  Tokens are organized into a parse tree, according to the syntax/grammar of the PL.  Parsing is recognition of a context-free language, the strings of which are token sequences that match a context-free grammar.  Theoretically, a parser is a pushdown automaton (PDA): a DFA with a stack. 1.37 Semantic analysis  The parse tree is traversed to understand its static characteristics.  Much of this analysis is type checking and conversion.  Execution (i.e., dynamic) behavior cannot be checked statically (e.g., array-indexing errors and expression values).  Theoretically, dynamic semantics is undecidable. 1.38 Intermediate code generation  The parse tree is again traversed to produce instructions for a virtual (i.e., imaginary and idealized machine).  A so-called intermediate form (IF) is often chosen for machine independence, ease of optimization, or compactness.  Java's byte code can be thought of as an intermediate form. 1.39 Machine-independent optimization  Optimization transforms an IF program into a faster or smaller one.  We're talking about CPU-independent optimizations (e.g., loop unrolling).  The term is a misnomer: \improvement" would be better.  Optimization is optional, often selected by a command-line argument (e.g., -O). 1.40 Code generation  Code generation transforms an IF program into assembly language for a particular CPU.  Some translators don't produce native code, just virtual code. 1.41 Machine-dependent code optimization  Some optimizations use CPU-specic features (e.g., special instructions or addressing modes).  Another example of target-dependent optimization is the reordering of machine instructions, allowing them to be executed in parallel. 1.42 Symbol table  All phases rely on a symbol table, which records information about the program's identiers (e.g., variables).  You can think of a symbol table as a stack of maps. Each map maps an identier (i.e., a string) to a record containing everything the translator knows about the identier (e.g., type and address). The stack represents nested scopes.  A symbol table may be retained, in some form, for use by a debugger, even after compilation. 1.43 A scanning example  Consider this implementation of Euclid's greatest common divisor algorithm: pub/ch1/gcd/gcd.c  These are the tokens, one per line: pub/ch1/gcd/tokens  For the scanner, the program might as well be: pub/ch1/gcd/ugly.c 1.44 A parsing example  A PL's parser parses according to a grammar for the language.  We'll learn more later, but a grammar is a set of rules, with a start symbol. Here's part of a grammar for C: pub/ch1/c-grammar  Here's a complete grammar for C: pub/ch1/c-grammar.y  Here's a complete grammar for Java: https://docs.oracle.com/javase/ specs/jls/se8/html/jls-19.html 1.45 A parse-tree example  This is just the top of the tree for the GCD program, according to the grammar: 1 translation_unit 2 |-external_declaration 3 | |-function_definition 4 | |-declaration_specifiers    5 | |-declarator    6 | |-declaration_list    7 | |-compound_statement    8 |-translation_unit 9 |-external_declaration 10 |-function_definition 11 |-declaration_specifiers    12 |-declarator    13 |-declaration_list    14 |-compound_statement    1.46 A syntax-tree example (1 of 2)  Since a parse tree is so huge, an abstract syntax tree (AST) it sometimes used, instead.  It omits information that is not needed after parsing.  It can even be fairly PL independent: an internal form.  This is for the GCD program's gcd() function denition: 1.47 A syntax-tree example (2 of 2) Symbol Table 1 gcd int int ! int 2 a int 3 b int 1.48 Important denitions (1 of 2)  An alphabet is a nite set of symbols (i.e., tokens/lexemes).  A string (i.e., program) is a sequence of symbols from an alphabet.  A language is a set of strings.  The syntax of a language is a set of rules that specify the set of strings in the language. For a PL, these strings are the set of \legal-looking" programs.  The semantics of a language is a set of rules that specify the meaning of each string in the language. For a PL, the semantics further species which programs are legal, and what a program \does" when it executes. 2.1 Important denitions (2 of 2)  This chapter is about syntax. The syntax of a formal language, like a PL, is specied at two levels, with two dierent notations: a regular expression (RE) and a (typically context-free) grammar (CFG).  The part of a translator that recognizes tokens, according to a regular expression, is called a scanner. It is a deterministic nite automaton (DFA).  The part of a translator that recognizes token sequences, according to a grammar, is called a parser. It is a push-down automaton (PDA). 2.2 Recursive denition of regular expressions (REs)  An RE is one of the following (in order of increasing precedence): { the empty string, denoted by  { an alphabetic character { two regular expressions separated by | (alternation) { two regular expressions juxtaposed (concatenation) { a regular expression followed by * (zero or more repetitions)  Parentheses allow grouping.  There are other operators (e.g., +), but this is the minimal set. 2.3 A regular-denition for numeric literals in Pascal  1  123  001  1.2  1.2e3  1.2e00 pub/ch2/numbers 2.4 Context-Free Grammars (CFGs)  The PL notation for CFGs is also called Backus-Normal Form or Backus-Naur Form (BNF).  A CFG has four parts: { The terminal symbols (aka, tokens) are a subset of the grammar symbols. Each represents a string of characters in a string in the language. { The nonterminal symbols are the rest of the grammar symbols. Each represents a string of terminals. { The start symbol is a nonterminal. { The productions each have a nonterminal on the left hand side (LHS) and a string of symbols (terminal and/or nonterminal) on the right hand side (RHS). 2.5 A CFG for simple expressions, with precedence and associativity  x  123  x*(12+-34)  ---123  ---(123) pub/ch2/exprs 2.6 Parse tree for 3+4*5 2.7 Parse tree for 10-4+3 2.8 Derivation of 3+4*5 1 expr => expr add_op term 2 => term add_op term 3 => factor add_op term 4 => number add_op term 5 => number '+' term 6 => number '+' term mult_op factor 7 => number '+' factor mult_op factor 8 => number '+' number mult_op factor 9 => number '+' number '*' factor 10 => number '+' number '*' number 2.9 Names, scopes, and bindings  A identier is a (hopefully) mnemonic character string representing something else (e.g., totalAmount).  A name is often just an identier, but can be more (e.g., x.y.z or Foo::bar).  A name provides abstraction. We can refer to the higher-level name (e.g., a variable), rather than the lower-level object it represents (e.g., its address).  Here, \object" has nothing to do with object orientation.  Abstraction is very important. It allows us to conquer complexity.  A binding is an association between two things, such as a name and the object it names.  The scope of a binding is the textual part of the program in which the binding is active.  An environment is a set of bindings. 3.1 Binding time  Binding time is the point at which a binding is created. More generally, it's the point at which any implementation decision is made. Here are some times and bindings: { PL design time: syntax, type system, and semantics { PL implementation time: CFG, input/output, and numeric precision. { program-development time: algorithms, design, and names { compile time: intra-module bindings and data layout { link time: inter-module bindings and whole-program layout { load time: virtual-memory addresses { run time: hardware addresses and variable values 3.2 Run time  program-startup time  module-entry time  elaboration time: when a declaration is rst \seen"  procedure-entry time  block-entry time  statement-execution time  procedure-exit time  program-exit time 3.3 Binding (1 of 2)  Static binding generally refers to one created before run time.  Dynamic binding refers to one created at or during run time.  Many of the important dierences between PLs are due to when various kinds of bindings are created (e.g., when a variable's type is bound). 3.4 Binding (2 of 2)  Earlier binding can improve performance.  Later binding can improve exibility.  Compiled PLs tend to have earlier binding.  Interpreted PLs tend to have later binding. 3.5 Scope  All PLs allow a programmer to name data. The name can then be used, rather than the data's address.  Not all data is named (e.g., some is referenced by pointers).  PLs have scope rules, which determine name/variable bindings. 3.6 Lifetime and storage-management events  creation of an object  creation of a binding  references to a variable, via a binding  temporary deactivation of a binding  reactivation of a binding  destruction of a binding  destruction of an object 3.7 Scope and lifetime  The time between the creation and destruction of a binding is called its lifetime (aka, extent).  The time between the creation and destruction of an object is its lifetime. They need not coincide.  If an object outlives all of its bindings it's garbage.  If a binding outlives its object it's a dangling reference.  The textual region of a program in which a binding is active is its scope.  Lifetime and scope rules vary signicantly across PLs. 3.8 Storage-allocation mechanisms  static: a xed-sized chunk of memory, reserved by the compiler/linker  stack: the CPU's call/return stack  heap: a managed pool of memory, from which run-time allocations and deallocations can occur (e.g., via Java's new keyword) 3.9 Static allocation  instructions and operands  global variables  static or own variables  literals and constants 3.10 Stack allocation  subroutine formal parameters and return variables  subroutine-local variables: scalars and aggregates  temporaries  block-local variables, which can reuse space pub/ch3/blockvars.c 3.11 Stack frames (1 of 2)  When a subroutine is called, or when a block is entered, the data pushed onto the stack is called a frame.  Frame content and organization is an important part of a translator's design, but a frame typically contains: { formal parameters { return values { local variables { scope information { return address { saved registers  A subroutine's code accesses stack data via osets from the beginning of the frame, which are computed at compile time.  A frame is bracketed by a frame pointer (fp) and the stack pointer (sp). 3.12 Stack frames (2 of 2)  Suppose main has called A, A has called B, B has called itself recursively, the second call to B has called C, and C is about to call D.  Today, our stack grows upwards, even though a push typically reduces the address in sp. 3.13 Stack-frame allocation and deallocation  Caller and callee both help maintain the callee's stack frame.  The caller's work is the calling sequence (e.g., pushing actual parameters).  The callee's initial work is the prolog (e.g., allocating space for local variables).  The callee's nal work is the epilog (e.g., deallocating space for the frame).  Finally, the caller retrieves the return values from just above the top of the stack, according to sp. This allows the caller to ignore the return values. 3.14 Heap allocation  During execution, a program can explicitly request (e.g., malloc()) an arbitrary-sized block of contiguous memory.  The allocated block is at least as big as the request. This causes internal fragmentation.  A reference (i.e., pointer) to the beginning of the block is returned.  The block's lifetime extends until: { explicit deallocation (e.g., free()) { no references to the block remain accessible to the program (i.e., it becomes garbage)  Since allocations and deallocations are temporally independent, free blocks cannot be completely coalesced. This causes external fragmentation. 3.15 Introduction to scope rules  A PL's scope rules determine the declaration bound to an identier.  The two broad styles are static (aka, lexical) scope and dynamic (aka, uid) scope. Static scope is far more common.  Traditionally, Lisp has dynamic scope, which appears to have been a \mistake." Scheme has static scope.  Bash has dynamic scope!  Some PLs have both (e.g., Common Lisp and Emacs Lisp).  First, we consider static scope. 3.16 Static Scope (1 of 8)  A scope is a portion of a program's source code, of maximal size, in which a binding does not change.  A scope need not be contiguous.  In most PLs, a scope is created upon subroutine (and sometimes block) entry: { create local-variable bindings { temporarily deactivate bindings for \shadowed" variables (creating a \hole" in the outer scope)  Upon subroutine/block exit: { destroy local-variable bindings { reactivate previously deactivated bindings 3.17 Static Scope (2 of 8)  An identier's declaration can be determined by examining a program's source code; you don't have to execute it.  In other words, a compiler can determine bindings.  Most PLs use static scope. Essentially, all compiled PLs do.  Basically, the last declaration of an identier, seen by the compiler, is the active one.  Most mainstream PLs (e.g., C, C++, Pascal, and Java) are descendants of the block-structured PL Algol 60, which uses static scope. 3.18 Static Scope (3 of 8)  In a block-structured PL, an identier is known (i.e., bound) in its declaration's block, and in each enclosed block, unless it is redeclared in an enclosed block.  To resolve a reference to an identier, check the scope of the referencing block, and then the scopes of statically enclosing blocks, until a binding is found. 3.19 Static Scope (4 of 8)  PLs with modules, classes, and/or packages have additional rules.  For example, in Java, the scope of a private static class variable is the entire enclosing class, even before the declaration, but its lifetime is an entire execution.  This is similar to an Algol own or C static local variable.  C example: pub/ch3/label.c 3.20 Static Scope (5 of 8)  Class-member visibility in Java: Modier Class Package Subclass World public Y Y Y Y protected Y Y Y N none Y Y N N private Y N N N  Which is missing? 3.21 Static Scope (5 of 8)  Recall that formal parameters and nonglobal variables are on the stack, in stack frames.  Immediately local variables are in the top frame.  Which frame contains a particular, visible, nonlocal variable? Where, exactly, is that frame? How does recursion complicate this problem?  Nested blocks in C: pub/ch3/blocks.c 3.22 Static Scope (6 of 8)  One solution: static links. { During compilation, compute the static nesting depth of each declaration (d) and reference (r). { During execution, maintain an entry in each frame, pointing to the nearest frame with the previous nesting depth. { To accomplish the reference, follow k = r ?? d static links, and access that frame.  A better solution: displays. { During compilation, compute the static nesting depth of each declaration (d). { Maintain an array of frame pointers, indexed by depth. { To accomplish the reference of a name declared at depth d, access the frame at display[d]. { For a same-depth (e.g., recursive) call, store the previous display entry in the frame, and restore it upon return. 3.23 Static Scope (7 of 8)  GCC extends C with nested function denitions.  Suppose we have this delightful program: pub/ch3/nonlocal.c and we are about to call printf(). 3.24 Static Scope (8 of 8)  Saying \a binding's scope is the block in which the declaration occurs" sounds simple, but there are complications: pub/ch3/order.c  Scheme provides three forms with varying semantics: let, let*, and letrec.  Some PLs allow declarations to appear anywhere in a block, treating them as if they are all at the beginning of a block, and allowing them to refer to themselves.  Other PLs (e.g., C, C++, and Pascal), allow \forward" declarations, which support recursive types (e.g., for a linked list).  Modules (e.g., classes) typically provide a semi-global/semi-local scope mechanism. 3.25 Dynamic Scope (1 of 2)  With static scope, bindings can be determined by looking at the source code.  With dynamic scope, the execution path must be known.  Bash examples: pub/ch3/dynamic1 pub/ch3/dynamic2  To resolve a reference, use the most recent active binding made at run time.  Dynamic scope is typically used in interpreted PLs with no compile-time type checking. 3.26 Dynamic Scope (2 of 2)  Suppose Java had dynamic scope: pub/ch3/Dynamic1.java pub/ch3/Dynamic2.java 3.27 